{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Titanic Disaster Data from Kaggle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /anaconda3/lib/python3.6/site-packages (0.8.2)\n",
      "\u001b[31mdistributed 1.21.8 requires msgpack, which is not installed.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# to use credentials in local .env we need to use this package\n",
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# walk the directories to get our env info\n",
    "dotenv_path = find_dotenv()\n",
    "#load up the entries as environment vairables \n",
    "load_dotenv(dotenv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graemer1975\n",
      "wat3rfall\n"
     ]
    }
   ],
   "source": [
    "# now let's get the username etc\n",
    "import os\n",
    "KAGGLE_USERNAME = os.environ.get(\"KAGGLE_USERNAME\")\n",
    "print(KAGGLE_USERNAME)\n",
    "KAGGLE_PASSWORD = os.environ.get(\"KAGGLE_PASSWORD\")\n",
    "print(KAGGLE_PASSWORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and that's how we get env info from our local machine for development, without sharing it on github by accident\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests import session\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the test and training data from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# login then download...\n",
    "payload = {\n",
    "    'action':'login',\n",
    "    'username': os.environ.get(\"KAGGLE_USERNAME\"),\n",
    "    'password': os.environ.get(\"KAGGLE_PASSWORD\")\n",
    "}\n",
    "\n",
    "\n",
    "# url for the training file - we could scrape this, but let's keep it simple right now\n",
    "url = 'https://www.kaggle.com/c/titanic/download/train.csv'\n",
    "loginUrl = \"https://www.kaggle.com/account/login\"\n",
    "\n",
    "## We have to login and accept all the checkboxes manually \n",
    "\n",
    "#set up the http session to connect and get the data\n",
    "with session() as c:\n",
    "    # post request\n",
    "    # c.post(\"https://www.kaggle.com/account/login\", data=payload)\n",
    "    \n",
    "    #NB this only works if we use antiforgery token as discussed in comments\n",
    "    response = c.get(loginUrl).text\n",
    "    AFToken = response[response.index('antiForgeryToken')+19:response.index('isAnonymous: ')-12]\n",
    "    #print(\"AntiForgeryToken={}\".format(AFToken))\n",
    "    payload['__RequestVerificationToken']=AFToken\n",
    "    c.post(loginUrl + '?IsModal=true&returnUrl=/', data=payload)\n",
    "    # get request\n",
    "    response = c.get(url)    \n",
    "    #print the response\n",
    "    #print(response.text)  #commented out, but it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sweet! we've got the train data - this works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### download and store the data locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import session\n",
    "# payload for login\n",
    "# login then download...\n",
    "payload = {\n",
    "    'action':'login',\n",
    "    'username': os.environ.get(\"KAGGLE_USERNAME\"),\n",
    "    'password': os.environ.get(\"KAGGLE_PASSWORD\")\n",
    "}\n",
    "\n",
    "loginUrl = \"https://www.kaggle.com/account/login\"\n",
    "\n",
    "def extract_data(url, file_path):\n",
    "    ''' \n",
    "    extract data from kaggle\n",
    "    '''\n",
    "    #as before, do some antiforgery on the session\n",
    "    #set up the http session to connect and get the data\n",
    "    with session() as c:\n",
    "        response = c.get(loginUrl).text\n",
    "        AFToken = response[response.index('antiForgeryToken')+19:response.index('isAnonymous: ')-12]\n",
    "        payload['__RequestVerificationToken']=AFToken\n",
    "        c.post(loginUrl + '?IsModal=true&returnUrl=/', data=payload)\n",
    "        # get request\n",
    "        #response = c.get(url)    \n",
    "        with open(file_path, 'wb') as handle:  #python3 needs wb, python2 only w\n",
    "            response = c.get(url, stream=True)\n",
    "            #print(response.text)\n",
    "            for block in response.iter_content(1024):  #capture the data in k sized chunks\n",
    "                handle.write(block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data source URLS\n",
    "train_url = 'https://www.kaggle.com/c/titanic/download/train.csv'\n",
    "test_url = 'https://www.kaggle.com/c/titanic/download/test.csv'\n",
    "\n",
    "#file paths for storing the data\n",
    "raw_data_path = os.path.join(os.path.pardir,'data','raw')\n",
    "train_data_path = os.path.join(raw_data_path,'train.csv')\n",
    "test_data_path = os.path.join(raw_data_path,'test.csv')\n",
    "\n",
    "#extract the data\n",
    "extract_data(train_url, train_data_path)\n",
    "extract_data(test_url, test_data_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 176\r\n",
      "-rw-r--r--  1 graemerenfrew  staff  28629 25 Jul 12:30 test.csv\r\n",
      "-rw-r--r--  1 graemerenfrew  staff  61194 25 Jul 12:30 train.csv\r\n"
     ]
    }
   ],
   "source": [
    "#is the data there?\n",
    "!ls -l ../data/raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excellent - this pulls the data down from Kaggle and stores it locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a script to do this\n",
    "### We want to be able to get this data via the command line, without having to use a jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_raw_data_script_file = os.path.join(os.path.pardir,'src','data','get_raw_data.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../src/data/get_raw_data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $get_raw_data_script_file\n",
    "# -*- coding: utf-8 -*-\n",
    "import requests\n",
    "from requests import session\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import logging #so we can show users what's happening\n",
    "\n",
    "# login then download...\n",
    "payload = {\n",
    "    'action':'login',\n",
    "    'username': os.environ.get(\"KAGGLE_USERNAME\"),\n",
    "    'password': os.environ.get(\"KAGGLE_PASSWORD\")\n",
    "}\n",
    "\n",
    "loginUrl = \"https://www.kaggle.com/account/login\"\n",
    "\n",
    "def extract_data(url, file_path):\n",
    "    ''' \n",
    "    extract data from kaggle\n",
    "    '''\n",
    "    #as before, do some antiforgery on the session\n",
    "    #set up the http session to connect and get the data\n",
    "    with session() as c:\n",
    "        response = c.get(loginUrl).text\n",
    "        AFToken = response[response.index('antiForgeryToken')+19:response.index('isAnonymous: ')-12]\n",
    "        payload['__RequestVerificationToken']=AFToken\n",
    "        c.post(loginUrl + '?IsModal=true&returnUrl=/', data=payload)\n",
    "        # get request\n",
    "        with open(file_path, 'wb') as handle:  #python3 needs wb, python2 only w\n",
    "            response = c.get(url, stream=True)\n",
    "            #print(response.text)\n",
    "            for block in response.iter_content(1024):  #capture the data in k sized chunks\n",
    "                handle.write(block)\n",
    "\n",
    "def main(project_dir):\n",
    "    '''\n",
    "    main method\n",
    "    '''\n",
    "    #get a logger\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.info('getting the raw data')\n",
    "    \n",
    "    # data source URLS\n",
    "    train_url = 'https://www.kaggle.com/c/titanic/download/train.csv'\n",
    "    test_url = 'https://www.kaggle.com/c/titanic/download/test.csv'\n",
    "    \n",
    "    #file paths for storing the data\n",
    "    raw_data_path = os.path.join(os.path.pardir,'data','raw')\n",
    "    train_data_path = os.path.join(raw_data_path,'train.csv')\n",
    "    test_data_path = os.path.join(raw_data_path,'test.csv')\n",
    "\n",
    "    #extract the data\n",
    "    extract_data(train_url, train_data_path)\n",
    "    extract_data(test_url, test_data_path)\n",
    "    logger.info('downloaded raw training and test data')\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    # get the root directory   - pardir is 'parent directory'\n",
    "    project_dir = os.path.join(os.path.dirname(__file__), os.pardir, os.pardir)\n",
    "    \n",
    "    #set up logger\n",
    "    log_fmt = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "    logging.basicConfig(level=logging.INFO, format=log_fmt)\n",
    "    \n",
    "    #now find the .env automatically by walking the directories\n",
    "    dotenv_path = find_dotenv()\n",
    "    #load the variables\n",
    "    load_dotenv(dotenv_path)\n",
    "    \n",
    "    #call the main methods\n",
    "    main(project_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-07-25 12:50:32,157 - __main__ - INFO - getting the raw data\n",
      "2018-07-25 12:50:38,054 - __main__ - INFO - downloaded raw training and test data\n"
     ]
    }
   ],
   "source": [
    "#now call the script via a shell command - remember to use python3, not just python\n",
    "!python3 $get_raw_data_script_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHOOOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
